import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class HTSF(nn.Module):
    def __init__(self, feature_dim, num_heads):
        """
        Initializes the HTSF module.
        Args:
            feature_dim: Dimension of the input features.
            num_heads: Number of attention heads for multi-head cross-attention.
        """
        super(HTSF, self).__init__()
        self.feature_dim = feature_dim
        self.num_heads = num_heads
        
        # Projection matrices for query, key, and value
        self.W_q = nn.Linear(feature_dim, feature_dim)
        self.W_k = nn.Linear(feature_dim, feature_dim)
        self.W_v = nn.Linear(feature_dim, feature_dim)

        # Attention normalization scaling factor
        self.scale = feature_dim ** 0.5

    def forward(self, spatial_feature, hemodynamic_features):
        """
        Performs the fusion of temporal hemodynamic features and spatial features.
        Args:
            spatial_feature (Tensor): Shape [B, C, H, W, D], spatial features from the segmentation network.
            hemodynamic_features (Tensor): Shape [T, B, C, H, W, D], temporal hemodynamic features.
        Returns:
            fused_feature (Tensor): Shape [B, C, H, W, D], fused feature map.
        """
        B, C, H, W, D = spatial_feature.shape
        T = hemodynamic_features.shape[0]

        # Flatten spatial features for attention calculation
        spatial_flat = spatial_feature.view(B, C, -1).permute(0, 2, 1)  # [B, HWD, C]
        
        # Process temporal features
        weighted_features = []
        for t in range(T):
            hemo_flat = hemodynamic_features[t].view(B, C, -1).permute(0, 2, 1)  # [B, HWD, C]

            # Calculate attention weights
            q = self.W_q(spatial_flat)  # [B, HWD, C]
            k = self.W_k(hemo_flat)    # [B, HWD, C]
            v = self.W_v(hemo_flat)    # [B, HWD, C]
            
            attn_weights = F.softmax(torch.bmm(q, k.transpose(-2, -1)) / self.scale, dim=-1)  # [B, HWD, HWD]
            weighted_hemo = torch.bmm(attn_weights, v)  # [B, HWD, C]

            # Reshape back to original spatial dimensions and add to list
            weighted_features.append(weighted_hemo.permute(0, 2, 1).view(B, C, H, W, D))

        # Aggregate weighted temporal features and fuse with spatial feature
        weighted_sum = sum(weighted_features) / T  # Temporal averaging
        fused_feature = weighted_sum + spatial_feature  # Add spatial feature

        return fused_feature


class CrossAttentionBlock(nn.Module):
    def __init__(self, spatial_channels, temporal_channels, attention_dim=64):
        super(CrossAttentionBlock, self).__init__()
        # use multi-head attention to fuse spatial and temporal features
        self.attn = nn.MultiheadAttention(embed_dim=attention_dim, num_heads=4)
        
        # use linear layers to project spatial and temporal features to attention_dim
        self.spatial_proj = nn.Linear(spatial_channels, attention_dim)
        self.temporal_proj = nn.Linear(temporal_channels, attention_dim)
        
    def forward(self, spatial_features, temporal_features):
        """
        Args:
            spatial_features (Tensor): spatial features (B, C, D, H, W)
            temporal_features (Tensor): HTSF Temporal features (B, T, C, D, H, W)
        """
        # flatten spatial and temporal features
        B, C, D, H, W = spatial_features.shape
        spatial_features = spatial_features.view(B, C, -1).permute(2, 0, 1)  # (D*H*W, B, C)
        temporal_features = temporal_features.view(B, -1, C, D, H, W).permute(1, 0, 2, 3, 4)  # (T, B, C, D, H, W)
        temporal_features = temporal_features.flatten(start_dim=3).permute(1, 0, 2)  # (B*T, C, D*H*W)
        
        # spatial and temporal projection
        spatial_proj = self.spatial_proj(spatial_features).permute(2, 0, 1)  # (D*H*W, B, attention_dim)
        temporal_proj = self.temporal_proj(temporal_features).permute(2, 0, 1)  # (B*T, C, attention_dim)
        
        # multi-head cross-attention
        attn_output, _ = self.attn(spatial_proj, temporal_proj, temporal_proj)
        
        # reshape and return
        output = attn_output.permute(1, 2, 0).view(B, C, D, H, W)
        return output

class DRDSNet(nn.Module):
    def __init__(self, ddpm, unet_seg, num_steps, device="cuda"):
        """
        DRDSNet includes a 3D DDPM and a 3D U-Net segmentation network.
        :param ddpm: 3D DDPM model
        :param unet_seg: 3D U-Net segmentation network
        :param num_steps: number of time steps generated by DDPM (S)
        :param device: running device
        """
        super(DRDSNet, self).__init__()
        self.ddpm = ddpm
        self.unet_seg = unet_seg
        self.num_steps = num_steps
        self.device = device

        self.attn_blocks = nn.ModuleList([
            HTSF(feature_dim=64),
            HTSF(feature_dim=128),
            HTSF(feature_dim=256),
            HTSF(feature_dim=512),
            HTSF(feature_dim=1024),
        ])


    def forward(self, input_mri, t_seq, beta=0, mris = None):
        """
        forward pass of DRDSNet
        :param input_mri: input MRI (B, C, D, H, W)
        :param t_seq: sequence of time steps
        :return: segmentation result (B, C_out, D, H, W)
        """
        b, c, d, h, w = input_mri.shape
        ddpm_features = []

        # 1. use DDPM to generate MRI sequence and extract features
        current_mri = input_mri
        DDPM_mris = []
        for t in range(0,t_seq):
            current_mri, features = self.ddpm_with_features(current_mri, t)
            ddpm_features.append(features)
            DDPM_mris.append(current_mri)
            current_mri = beta * mris[t] + (1 - beta) * current_mri


        # 2. use segmentation network with multiple features from DDPM
        seg_output = self.unet_seg_with_ddpm_features(input_mri, ddpm_features)
        return seg_output, DDPM_mris

    def ddpm_with_features(self, x, t):
        """
        use DDPM to generate a single MRI sequence and extract features
        :param x: input MRI (B, C, D, H, W)
        :param t: current time step
        :return: generated MRI, encoder downsampled features
        """
        
        features = []
        x1 = self.ddpm.inc(x)
        features.append(x1)
        x2 = self.ddpm.down1(x1, t)
        features.append(x2)
        x3 = self.ddpm.down2(x2, t)
        features.append(x3)
        x4 = self.ddpm.down3(x3, t)
        features.append(x4)
        x4 = self.ddpm.bot1(x4)
        x4 = self.ddpm.bot2(x4)
        x4 = self.ddpm.bot3(x4)
        x = self.ddpm.up1(x4, x3, t)
        x = self.ddpm.up2(x, x2, t)
        x = self.ddpm.up3(x, x1, t)
        output = self.ddpm.outc(x)
        return output, features

    def unet_seg_with_ddpm_features(self, x, ddpm_features):
        """
        use segmentation network with multiple features from DDPM
        :param x: input MRI (B, C, D, H, W)
        :param ddpm_features: multiple downsampled features from DDPM
        :return: segmentation result (B, C_out, D, H, W)
        """
        x1 = self.unet_seg.inc(x)
        x1 = self.attn_blocks[0](x1, [x[0] for x in ddpm_features])
        x2 = self.unet_seg.down1(x1)
        x2 = self.attn_blocks[0](x2, [x[1] for x in ddpm_features])
        x3 = self.unet_seg.down2(x2)
        x3 = self.attn_blocks[0](x3, [x[2] for x in ddpm_features])
        x4 = self.unet_seg.down3(x3)
        x4 = self.attn_blocks[0](x4, [x[3] for x in ddpm_features])
        x4 = self.unet_seg.bot1(x4)
        x4 = self.unet_seg.bot2(x4)
        x4 = self.unet_seg.bot3(x4)
        x = self.unet_seg.up1(x4, x3)
        x = self.unet_seg.up2(x, x2)
        x = self.unet_seg.up3(x, x1)
        output = self.unet_seg.outc(x)
        return output

